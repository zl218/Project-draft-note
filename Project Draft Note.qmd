---
title: "Project Draft note"
subtitle: "Non-Gaussian and Gaussian Process for Interpolation"
author: Fuad Hasan and Zijie (Ian) Liang
date: "2025-11-10"
type: "Project"
objectives:
  - "Show the interpolation definition"
  - "Inverse Distance Weighting Interpolation"
  - "KNN Method"

engine: julia

format:
  html:
    toc: true
    toc-depth: 2
    code-block-bg: "#f8f8f8"
    code-block-border-left: "#e1e4e5"
    theme: simplex
    number-sections: true
    fig-format: png
  typst:
    fontsize: 11pt
    margin:
      x: 1in
      y: 1in
    number-sections: true
    fig-format: png

execute:
  cache: true
  freeze: auto

# Code formatting options
code-overflow: wrap
code-line-numbers: false
code-block-font-size: "0.85em"
---

# Code Setup

Begin with project management:

```{julia}
#| output: false
using Pkg
lab_dir = dirname(@__FILE__)
Pkg.activate(lab_dir)
Pkg.instantiate() # uncomment this the first time you run the lab to install packages, then comment it back
```

Load all required packages:

```{julia}
using Distances # use to compute pairwise Euclidean distance
using Interpolations
using Distributions
using GaussianProcesses # for the useful syntax
using LaTeXStrings
using LinearAlgebra
using Optim
using Plots
using Random
```


# Background and Reading

This is the draft note for our porject. In this draft we discussed the important contents included the definition of interpolation, multiple interpolation methods, and 1D Gaussian method for the interpolation.


# Interpolation Motivation

When testing the environmental data, sometimes we are interested in the nearby data. The normal operation is to used the observation to predict/simulate the surroundance data that we haven't observed. Take the following examples:
1. Rainfall: Only a few rain gauges cover a watershed, but we need the rainfall data for the whole watershed to drive hydrologic models;
2. Temperature or Air Quality: Satellite retrievals are patchy or have missing pixels so we need to fill gaps;
3. Urban Planning/Infrastructure: Sensor data (traffic, vibration, flood depth) are discrete but need continuuous spatial representation.

## Example

With the help by the note from Dr. James Doss-Gollin's previous class, we generate some basic model as example to show the interpolation example and basic concept.

```{julia}
f(t) = @. sin(2 * pi * t / 7.2 + 0.9) + sin(2 * pi * t / 2/5 + 1.3)

begin
    N = 25            # number of observations
    σy = 0.25         # noise parameter
    x0 = 0            # lower bound
    x1 = 10           # upper bound
    Random.seed!(70005)              # set seed
    x = sort(rand(Uniform(x0, x1), N))      # points we observe at
    xprime = collect(range(x0, x1, length=1000))   # locations to estimate
    y = f.(x) .+ rand(Normal(0, σy), N)            # observed values with noise
    baseplot = plot(xlabel=L"$x$", ylabel=L"$y(t)$", legend=:bottomright)
end

p = deepcopy(baseplot)
plot!(p, xprime, f.(xprime), label="True Values")
scatter!(p, x, y, label="Observed Points")

```

Without the blue line, how do we predict the point between `x = 2.5` and `x = 5.0`? We could use interpolation to simulate the observation.

# Non-Gaussian Process

## Interpolation

In order to do the prediction, the simplest way is to try a linear interpolation. Linear interpolation estimates the value of a function between two known data points by assuming the change between them is linear (a straight line).
If the function is known at points $(x_1, y_1)$ and $(x_2, y_2)$, then for any $x$ between them:
$$y(x) = y_1 + \frac{(y_2 - y_1)}{(x_2 - x_1)} (x - x_1)$$

The most important thing here is connecting the dots. If we have high density data, which are unbiased estimates, the model works well. However, if the data size is small, the predition will not perfom well.

```{julia}
itp_linear = LinearInterpolation(x, y; extrapolation_bc = Line())
f_linear(x) = itp_linear(x)
p = deepcopy(baseplot)
scatter!(p, x, y, label = "Observed")
plot!(p, f, x0, x1, label = L"$f(x)$")
plot!(f_linear, xprime, label = "Linear interpolation")

```


## Inverse Distance Weighting Interpolation

Inverse Distance Weighting (IDW) is a simple and widely used spatial interpolation method.  
It predicts the value at an unknown location as a `weighted average` of known data points,  
where weights depend on the `distance` between points — closer points have higher influence.

Mathematically, the predicted value $\hat{z}(x_0)$ at location $x_0$ is given by:

$$
\hat{z}(x_0) = \frac{\displaystyle\sum_{i=1}^{N} w_i \, z(x_i)}{\displaystyle\sum_{i=1}^{N} w_i}
$$

where the weights $w_i$ are defined as:

$$
w_i = \frac{1}{d(x_0, x_i)^p}
$$

Here:  
- $z(x_i)$ = known value at location $x_i$  
- $d(x_0, x_i)$ = distance between $x_0$ and $x_i$ (usually Euclidean)  
- $p$ = power parameter controlling how quickly influence decreases with distance  
  - Small $p$ → smoother interpolation  
  - Large $p$ → closer points dominate  

The weights are normalized so that they sum to 1:

$$
\sum_{i=1}^{N} w_i = 1
$$

IDW is deterministic and easy to compute, but it assumes the spatial relationship depends only on distance —  
it does not provide uncertainty estimates or account for directional trends.

```{julia}
calc_dist(x1, x2) = Distances.pairwise(Distances.Euclidean(), x1, x2)

function idw(x, y, xprime) where T <: Real
 dist = Distances.pairwise(Distances.Euclidean(), x, xprime)
 weights = dist .^ (-2)
 weights_norm = weights ./ sum(weights, dims = 1)
 return weights_norm' * y
    
end

yprime = idw(x, y, xprime)
p = deepcopy(baseplot)
plot!(p, f, x0, x1, label = "Ture values")
scatter!(p, x, y, label = "observed Points")
plot!(p, xprime, yprime, label = "Predicted (IDW)")
```

Since we have some good observations, the model prediction is really well. There are some weird vibration due to the high concentration of the data in a small area. 

## K Nearest Neighbors (KNN)

`K Nearest Neighbors (KNN)` is a variation of the Inverse Distance Weighting (IDW) method.  
Instead of using *all* available data points, KNN considers only the `K closest points` around the prediction location.  
This helps reduce the influence of faraway points, making the interpolation more local and often more accurate.

Let $\hat{z}(x_0)$ be the predicted value at an unknown location $x_0$.  
Among all $N$ known points $(x_i, z(x_i))$, we first find the $K$ nearest neighbors based on the distance $d(x_0, x_i)$.

The prediction is then computed as a weighted average over these $K$ neighbors:

$$
\hat{z}(x_0) = 
\frac{\displaystyle\sum_{i=1}^{K} w_i \, z(x_i)}
     {\displaystyle\sum_{i=1}^{K} w_i}
$$

where the weights are defined using the same distance-decay principle as IDW:

$$
w_i = \frac{1}{d(x_0, x_i)^p}
$$

Here:  
- $d(x_0, x_i)$ = distance between the prediction point and the $i$-th neighbor  
- $p$ = power parameter controlling the rate of distance decay  
- $K$ = number of nearest neighbors used for interpolation  

By setting all weights outside the $K$ nearest points to zero before normalization,  
the KNN interpolation limits the effect of distant data, improving computational efficiency  
and reducing unrealistic “global” influence in sparse datasets.

```{julia}
function knn(x, y, xprime, K) where T <: Real
 dist = Distances.pairwise(Distances.Euclidean(), x, xprime)
 weights = dist .^ (-2)

 # truncate some weights to zero
ranks = hcat([invperm(sortperm(col; rev = true)) for col in eachcol(weights)]...)
    weights[ranks .> K] .= 0

    # normalize
    weights_norm = weights ./ sum(weights, dims = 1)
    return weights_norm' * y

end

```

So now with $K$ = $N$, the IDW estimation will be
```{julia}
K = N
yprime = knn(x, y, xprime, K)
p = deepcopy(baseplot)
plot!(p, f, x0, x1, label = "True Values", title = "K=N")
scatter!(p, x, y, label = "Observed Points")
plot!(p, xprime, yprime, label = "Prediction (KNN)")

```

What if we reduce the number of $K$, the estimation will be changed again
```{julia}
K = 3
yprime = knn(x, y, xprime, K)
p = deepcopy(baseplot)
plot!(p, f, x0, x1, label = "True Values", title = "K=$K")
scatter!(p, x, y, label = "Observed Points")
plot!(p, xprime, yprime, label = "Prediction (KNN)")

```

The prediction line looks wired when the `K` value changed. There are another method to run the interpolation like the high-order polynomial. However, one of the best way for the estimation is `Gaussian Processes`.

# 1D Gaussian process

Until this portion, we have seen the non-gaussian process.The orange dots were the observed points that has obviously some noise(as say discahrge measurement will have some noise, a discharge value we are saying 22, maybe 22.4) and blue line in the previous graphs was the true underlying function that can fit the observed data best(say fitting the discharge value). But, in reality we actually don't know how the blue line will look like.  And also, in most regression methods, we assume that y = aX + b. The thing is that, all of these methods just give a single best-fit curve (green curves). Here, the Gaussian process comes handy. We can call it a Bayesian way of modeling nonlinear relationships. It allows us to fit a family of smooth curves to the data, rather than trying to fit the data with a single line (i.e., in linear regression), while also providing a measure of uncertainty at every point.

What are the advantages of a Gaussian process?
## Say with some real example, we have some air temperature readings in some stations. But we want to estimate the temperature between two stations. Or, say, we measure river discharge at some points at times t = 1, 3, and 5 hours. But we want to know what happened at t=2 and 4 hours(testing/prediction points)? In the first case, the nearby points will have a similar kind of temperature (spatial relation), and in case number 2, the nearby times will have a similar kind of discharge (temporal correlation).

So, in this kind of scenario, a Gaussian process can come in handy. If we have some observed data, and we want to predict what happened in unknown data points, this GP process can:
1.  Naturally handle the spatial and temporal relationship.
2.  Can provide smooth interpolation between known points.
3.  Also, can quantify uncertainty.
What do we understand from this? In general, the GP doesn’t assume that there is a fixed equation like say y=aX+b, rather, they consider the function(function means it randomly take some curve from gaussian distribution) itself to be random, nearby points are correlated (determined through a kernel function), to smoothly predict what is happening between prediction/testing data points where we don’t have any observation, and show how uncertain those predictions are.

## Steps of 1D Gaussian Process
### 1. GP Prior Assumption

We assume the function $f(x)$ we’re trying to learn is drawn from a **Gaussian Process**:

$$
f(x) \sim \mathcal{GP}\big(m(x), K(x, x')\big)
$$

where  

- $m(x)$ is the **mean function**, often assumed constant ($\mu$),  
- $K(x, x')$ is the **covariance (kernel) function**, measuring similarity between inputs.  

So, for any finite set of inputs  

$$
x = [x_1, ..., x_N], \quad x' = [x'_1, ..., x'_{N'}],
$$  

the **joint distribution** of function values is multivariate normal:

$$
\begin{bmatrix}
y' \\
y
\end{bmatrix}
\sim
\mathcal{N}\left(
\begin{bmatrix}
a \\
b
\end{bmatrix},
\begin{bmatrix}
A & B \\
B^T & C
\end{bmatrix}
+ \sigma_y^2 I
\right)
$$




##  2. Meaning of Each Symbol

| **Symbol** | **Meaning** | **Size** |
|:------------|:------------|:----------|
| $y$ | training outputs | $N \times 1$ |
| $y'$ | test outputs | $N' \times 1$ |
| $a = m(x')$ | mean at test points | $N' \times 1$ |
| $b = m(x)$ | mean at training points | $N \times 1$ |
| $A = K(x', x')$ | test–test covariance | $N' \times N'$ |
| $B = K(x', x)$ | test–train covariance | $N' \times N$ |
| $C = K(x, x)$ | train–train covariance | $N \times N$ |
| $\sigma_y^2 I$ | observation-noise term | $N \times N$ |


### 3. Kernel Function

We use the **squared-exponential (RBF)** kernel:

$$
K(x, x' \mid \sigma, \ell)
= \sigma^2 \exp\left[
-\frac{(x - x')^2}{2\ell^2}
\right]
$$

where:

- $\sigma^2$: **signal variance** (controls amplitude) - How high or low a curve can go  
- $\ell$: **length scale** (controls smoothness — small $\ell$ means wiggly function that change rapidly, large $\ell$ means smooth function changing slowly)...
$\el$ if x and x' are close enough, that means points $\el$ will have high covariance, points that are far, $\el$ will have low covariance

## 4.6.3 Kernel Parameters

Of course, we don’t initially know what **σ (signal variance)** or **ℓ (length scale)** should be — 
we just guess some starting values. The Gaussian Process then uses the training data to 
automatically adjust these parameters so that the model best explains the observations. 
This process is called *optimization*: the GP tries different values of σ and ℓ, checking 
which ones make the data most likely under the model. The search continues until the fit 
stops improving. To keep the optimization stable and ensure both parameters remain positive, 
we usually optimize over **log(σ)** and **log(ℓ)** instead of their direct values. 
The final optimized σ and ℓ describe how variable and how smooth the data is, respectively.

## 4.6. 4. Prediction (Posterior Distribution)

The conditional (posterior) distribution of the **test outputs** `y′` given the **training outputs** `y` is **Gaussian**:

$$
p(y'|y) = \mathcal{N}(m, S)
$$

where:

$$
m = a + B(C + \sigma_y^2 I)^{-1}(y - b)
$$

$$
S = A - B(C + \sigma_y^2 I)^{-1}B^T
$$

Here:

- **m** → The **posterior mean** — it gives the predicted average function value at each test point.  
  (This is the black “mean curve” we plot in a Gaussian Process.)
  
- **S** → The **posterior covariance matrix** — it gives the uncertainty between predicted points.  
  (The diagonal of **S** determines the width of the shaded uncertainty region around the mean.)


In Gaussian Process, we assume the function has a constant mean (μ), meaning both training and testing points share the same average value. This helps the model focus on variations around this mean rather than the overall level. 
μ = mean(y)   # compute mean from training data.
a = μ         # mean for test points.
b = μ         # mean for training points

